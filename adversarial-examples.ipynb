{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial examples\n",
    "\n",
    "V tomto cvičení si vyzkoušíme ošálit natrénovanou konvoluční síť malým pozměněním vstupního obrázku tak, aby výsledné skóre bylo maximální pro nějakou námi zvolenou třídu. Pokud např. neuronová síť správně klasifikuje obrázek automobilu s výstupní pravděpodobností 99 %, cílenou, pro lidské oko však téměř neznatelnou úpravou můžeme dosáhnout 99 % výstupní pravděpodobnosti např. pro třídu jelen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from skimage import measure\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "import ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 12., 8.\n",
    "plt.rcParams['image.interpolation'] = 'nearest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Útočit budeme na předtrénovaný model ze ZOO knihovny PyTorch modulu torchvision, který je naučen kasifikovat obrázky databáze ImageNet. Použití modelu z torchvison ZOO je velmi jednoduché. Pokud chceme použít síť s již natrénovanými parametry, stačí do konstruktoru zadat parametr `pretrained=True` a váhy se automaticky stáhnou z repozitáře a uloží do domovského adresáře do podsložky `.torch`. Obvykle se jedná o několik desítek až stovek MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = torch.models.resnet18(pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pokud chceme akcelerovat výpočty na grafické kartě:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.cuda();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jelikož model obsahuje vrstvy, které se chovají různě v trénovací a testovací fázi, nesmíme zapomenout přepnout model do testovacího režimu. Jinak může být výstup často nesmyslný, což si ostatně můžete vyzkoušet sami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model navíc nebudeme učit a pro zrychlení výpočtu úplně zablokujeme výpočet gradientů na jeho parametry. Síť zde vystupuje jako sice diferencovatelná, ale jinak neměnná funkce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for par in model.parameters():\n",
    "    par.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data tentokrát potřebovat nebudeme. Stačit nám bude jediný obrázek, např. naše oblíbená žabička."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rgb = cv2.imread('./data/happy-green-frog.jpg')[..., ::-1]\n",
    "test_rgb.dtype, test_rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Všechny modely z torchvision ZOO natrénované na ImageNet na vstupu očekávají RGB obrázky o rozměrech 224 x 224 pixelů. Jelikož budeme potřebovat gradient na vstupní obrázek, bude snazší pracovat s touto velikostí. Ve druhé části cvičení pak toto omezení odstraníte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rgb_small = cv2.resize(test_rgb, (224, 224))\n",
    "test_rgb_small.dtype, test_rgb_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_rgb_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ImageNet dělí objetky do 1000 tříd, jejichž názvy si načteme ze souboru. Seznam je volně dostupný [zde](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dutch oven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classes = [re.search('[\\'\"](.+?)[\\'\"]', l).group(1) for l in open('./data/imagenet1000_clsidx_to_labels.txt')]\n",
    "print('\\n'.join(f'{i}: {c}' for i, c in enumerate(classes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Předzpracování obrázku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Všechny modely ze ZOO knihovny torchvision natrénované na ImageNet aplikují na vstupní obrázek shodné úpravy. Kromě normalizace na rozlišení 224 x 224 pixelů se rovněž hodnoty pixelů standardizují vydělením 255, odečtením průměrného RGB pixelu a vydělením standardní odchylkou pro každý kanál zvlášť. Průměrný pixel i standardní odchylka byly vypočteny z celého ImageNetu a jsou pro všechny modely a obrázky společné (jedná se o konstanty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = transforms.Compose([\n",
    "    transforms.ToTensor(),  # prevede z numpy.ndarray na torch.Tensor a zaroven vydeli 255\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # normalizuje\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abychom mohli upravený obrázek zobrazit, potřebujeme inverzní transformaci, která obrázek vrátí do rozsahů a datových typů vhodných pro matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprep = transforms.Compose([\n",
    "    lambda x: x.detach().cpu().numpy(),\n",
    "    lambda x: x.transpose(1, 2, 0),\n",
    "    lambda x: x * np.array([0.229, 0.224, 0.225]),\n",
    "    lambda x: x + np.array([0.485, 0.456, 0.406]),\n",
    "    lambda x: np.uint8(255 * x.clip(0., 1.))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vstupní tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vyzkoušíme, zdali síť funguje. Funkce `ans.predict_and_show(rgb, model, transform)` vezme obrázek `rgb`, upraví ho transformací `prep` a klasifikuje modelem `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans.predict_and_show(test_rgb_small, model, prep, classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak bylo zmíněno, na rozdíl od učení modelu pro klasifikaci tentokrát nebudeme pro dosažení kýženého výstupu upravovat parametry, ale samotný vstup. Musíme proto vytvořit tensor obrázku tak, aby PyTorch věděl, že na něj má počítat gradient. Nejprve však transformujeme standardizujeme odečtením průměru a vydělením odchylkou, jak si to síť \"přeje\", viz výše."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_input_small = prep(test_rgb_small)[None]  # `[None]` pro pridani batch dimenze\n",
    "adv_input_small.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ujistíme se, aby tensor byl ve stejném zařízení (CPU vs GPU), jako je model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adv_input_small = adv_input_small.to(next(model.parameters()).device)\n",
    "adv_input_small.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na tensor `adv_input_small` zavoláme metodu `requires_grad_`, jež PyTorchi sdělí, že při zpětném průchodu požadujeme gradient i na tento vstup výpočetního grafu (defaulntě je `False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_input_small.requires_grad_(True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Výstupní skóre a pravděpodobnost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak již bylo řečeno, ze sítě dostaneme vektor 1000 skóre pro jednotlivé třídy. Pravděpodobnosti pak získáme jednoduše aplikací softmaxu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model(adv_input_small)\n",
    "scores.dtype, scores.shape, scores.min(), scores.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = softmax(scores)\n",
    "probs.dtype, probs.shape, probs.min(), probs.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadání"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upravte `adv_input_small` tak, aby obrázek byl s alespoň 99% pravděpodobností klasifikován jako libovolná jiná třída. Cílovou kategorii zvolte tak, aby byla vizuálně velmi vzdálená třídám dosahujícím (relativně) vysoké skóre na neupraveném obrázku (tree frog, bullfrog, green lizard, chameleon, ...). Výsledný adversarial obrázek přitom musí zůstat vizuálně téměř nerozeznatelný od původního.\n",
    "\n",
    "**Postup:**\n",
    "- síti je třeba nějak sdělit, že na obrázku není `žába`, ale např. `samice hrabáče`\n",
    "- možností je více:\n",
    "  - standardní cross entropy loss, kdy správná třída $y$ je nastavena na index třídy `samice hrabáče`\n",
    "  - bez lossu, kdy zpětný průchod začne až od výstupního skóre (každá tensor v PyTorch implemetuje metodu `backward`), přičemž gradient shora je nastaven na vektor $[-1, -1, \\ldots, +1, \\ldots, -1]$; tento postup je častější a bude fungovat lépe\n",
    "- vypočteným gradientem updatujte vstup\n",
    "  - buď ručně - každá proměnná vytvořená s `requires_grad=True` po zavolání `backward` obsahuje `grad` (pokud tato proměnná je součástí dynamického výpočetního grafu),\n",
    "  - nebo skrze PyTorch optimizer jako při trénování sítě\n",
    "- úprava je podobný proces jako trénování sítě, tj. opakujeme uvedené kroky, dokud výstupní pravděpodobnost pro třídu `samice hrabáče` není alespoň 99 %\n",
    "\n",
    "Výsledek zapište do proměnné `adv_input_small`. Rozdíl $MSE(x, a)$ mezi původním $x$ a upraveným obrázkem $a$ musí být $MSE(x, a) < 1$.\n",
    "\n",
    "**Pozn.:** Vzhledem ke zaokrouhlovacím chybám a dalším úpravám pravděpodobnost ukazovaná funkcí `ans.predct_and_show` nebude přesně odpovídat pravděpodobnosti ze softmaxu z optimalizačního procesu, ale nejspíše bude o něco málo menší. Nezsavujte proto \"učící\" proces, hned jak je objeven adversarial obrázek, ale až o chvilku déle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# ZDE DOPLNIT\n",
    "# (pridejte libovolny pocet bunek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_rgb_small = unprep(adv_input_small.detach()[0])\n",
    "adv_rgb_small.dtype, adv_rgb_small.shape, adv_rgb_small.min(), adv_rgb_small.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predikce na adversarial obrázku:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans.predict_and_show(adv_rgb_small, model, prep, classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Původní, adversarial a jejich rozdíl vedle sebe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(test_rgb_small)\n",
    "plt.title('původní')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(np.uint8(128 + test_rgb_small.astype(np.float) - adv_rgb_small.astype(np.float)))\n",
    "plt.title(f'rozdíl (MSE={measure.compare_mse(test_rgb_small, adv_rgb_small):.2f})')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('adversarial')\n",
    "plt.imshow(adv_rgb_small)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Původní rozlišení"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adversarial lze vytvořit i v původním rozlišení, bez transformace na velikost 224x224 pixelů. Jediné, co k tomu potřebujeme, je nahradit funkci `cv2.resize` diferencovatelnou operací takovou, pro kterou funguje PyTorch autograd pro automatický výpočet gradientu na vstup (backprop). Prohledejte dokumentaci, najděte vhodnou funkci a zopakujte předchozí postup na obrázek v původním rozlišením tj. `test_rgb` (a odpovídající `adv_input`) místo `test_rgb_small` (`adv_input_small`). Výstupem bude `adv_rgb` namísto `adv_rgb_small`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_input = prep(test_rgb)[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_input = adv_input.to(next(model.parameters()).device)\n",
    "adv_input.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_input.requires_grad_(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# ZDE DOPLNIT\n",
    "# (pridejte libovolny pocet bunek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_rgb = unprep(adv_input.detach()[0])\n",
    "adv_rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ans.predict_and_show(cv2.resize(adv_rgb, (224, 224)), model, prep, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(test_rgb)\n",
    "plt.title('původní')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(np.uint8(128 + test_rgb.astype(np.float) - adv_rgb.astype(np.float)))\n",
    "plt.title(f'rozdíl (MSE={measure.compare_mse(test_rgb, adv_rgb):.1f})')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('adversarial')\n",
    "plt.imshow(adv_rgb)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0d497edcb885431fadaa9dc79fce142d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_875c42922d884ebea23853d7212f286b",
        "IPY_MODEL_f32e4002b26241328a16ca43ef710aa7"
       ],
       "layout": "IPY_MODEL_56c5030b496d446db2ea771a91fc300e"
      }
     },
     "1527cb0e28d2464bb051ddd8c539b8a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "331dd1448ff24bfaa619213beac8c0f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "3684af1e44774216be344e2b07808f1f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4161c4d88f744453b64d6ce0c567db96": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_8eeaf38c6cd741cc88a74a92610f6852",
       "style": "IPY_MODEL_331dd1448ff24bfaa619213beac8c0f7",
       "value": " 18% 184/1000 [00:04&lt;00:19, 41.60it/s]"
      }
     },
     "56c5030b496d446db2ea771a91fc300e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "60289f50425b4e79830a61ddfa0237c1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "875c42922d884ebea23853d7212f286b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "danger",
       "layout": "IPY_MODEL_9e5f74e46b5740de99ff70f436a0e81a",
       "max": 1000,
       "style": "IPY_MODEL_fb52f79b0e504c86afee41b981ffba31",
       "value": 397
      }
     },
     "8eeaf38c6cd741cc88a74a92610f6852": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9e5f74e46b5740de99ff70f436a0e81a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9f353b3ad8c34193b7be9af848efa073": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b172890d56b54e87a959d85bc52a8ee5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_df52df57b6994a50a6b2c52ab058144c",
        "IPY_MODEL_4161c4d88f744453b64d6ce0c567db96"
       ],
       "layout": "IPY_MODEL_60289f50425b4e79830a61ddfa0237c1"
      }
     },
     "b678107275a9476d9c81a5a46d41ec3d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "df52df57b6994a50a6b2c52ab058144c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "danger",
       "layout": "IPY_MODEL_3684af1e44774216be344e2b07808f1f",
       "max": 1000,
       "style": "IPY_MODEL_1527cb0e28d2464bb051ddd8c539b8a6",
       "value": 184
      }
     },
     "f32e4002b26241328a16ca43ef710aa7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9f353b3ad8c34193b7be9af848efa073",
       "style": "IPY_MODEL_b678107275a9476d9c81a5a46d41ec3d",
       "value": " 40% 397/1000 [00:09&lt;00:14, 42.29it/s]"
      }
     },
     "fb52f79b0e504c86afee41b981ffba31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
