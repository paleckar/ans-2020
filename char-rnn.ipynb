{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generování textu znakovou RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "import ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V tomto cvičení nebudeme používat GPU, protože budeme zpracovávat znaky po jednom a v takto malých dávkách overhead způsobený neustálými přesuny dat mezi GPU a RAM výpočty pouze zpomalí."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Namísto obrazu tentokrát použijeme textová data. Konkrétně se jedná o novinové nadpisy, které se budeme snažit generovat automaticky. Všechna data jsou v jediném souboru `data/headlines.txt`.\n",
    "\n",
    "Z textu byly odstraněny háčky, čárky a všechny nestandardní znaky. Není tedy potřeba řešit kódování apod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('data/headlines.txt').read()\n",
    "lines = [line for line in data.split('\\n') if line]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ukázka dat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(i, random.choice(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sada znaků = náš slovník:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(sorted(set(data)))\n",
    "print(len(chars), chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Všimněme si, že první znak je `'\\n'`. Ten použijeme jako stop znak, neboli speciální token, jenž bude označovat konec sekvence. Pokud tedy při postupném generování věty dojde na tento znak, proces zastavíme.\n",
    "\n",
    "Následující tabulka (`dict`) nám usnadní převod znaku na index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr2idx = {c: i for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podíváme se na statistické rozložení prvních znaků ve větách."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {c: 0 for c in chars}\n",
    "for line in lines:\n",
    "    counts[line[0]] += 1\n",
    "counts = np.array([counts[c] for c in chars], dtype=np.float)\n",
    "p0 = counts / counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "rects = plt.bar(range(len(chars)), 100. * p0)\n",
    "plt.xticks(range(len(chars)), ['{}'.format(repr(c)) for c in chars])\n",
    "for r in rects:\n",
    "    x, w, h = r.get_x(), r.get_width(), r.get_height()\n",
    "    plt.text(x + w / 2., h + 0.1, '{:.1f}'.format(h), ha='center', va='bottom', fontsize=8)\n",
    "plt.ylabel('počet');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sekvenční data a PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Následující funkce `str2idt` převede řetězec na sekvenci čísel (index tensor) odpovídajících indexům znaků v tabulce. Pokud např. bude celý náš \"slovník\" `chars = ['a', 'b', 'c']`, pak funkce `str2idt` převede řetězec `'acba'` na `[0, 2, 1, 0]`. Výsledek vrátí jako PyTorch `torch.Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2idt(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = chr2idx[string[c]]\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nas slovnik ma 38 znaku, takze indexy znaku budou jine\n",
    "x = str2idt('abca')\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Další funkce bude dělat opak: převede sekvenci indexů na řetězec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idt2str(indices):\n",
    "    return ''.join([chars[i] for i in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idt2str(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekvenci čísel potřebujeme převést na vektory jednotlivých znaků. Tento proces se v anglické literatuře označuje jako embedding a PyTorch ho implementuje jako vrstvu třídou `Embedding`. Vyjádřením této operace diferencovatelnou vrstvou umožňuje učení vektorů, které tedy nemusejí být fixní. O tom ale až příště."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# velikost slovniku je `len(chars)`\n",
    "# dimenze znakoveho vektoru bude napr. 30\n",
    "emb = nn.Embedding(len(chars), 30)\n",
    "\n",
    "# dopredny pruchod\n",
    "e = emb(x)\n",
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Embedding` nedělá nic jiného, než že na výstup pro znak s indexem $i$ vrátí $i$-tý řádek své váhové matice `weight`, která drží vektory slov. Defaultně je tato matice inicializována náhodně. Pokud první písmeno v příkladu bylo 'a', jehož index ve \"slovníku\" `chars` je 12, první řádek embeddingu `e` bude odpovídat 13. řádku (index 12) matice `emb.weight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool(torch.all(e[0] == emb.weight[12]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN v PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch implementuje tři z nejrozšířenějších typů sítí třídami `RNN`, `LSTM` a `GRU`. API je pro všechny stejné: dopředný průchod `forward` očekává \"zespodu\" nějaký vstup `input` a \"zleva\" minulý stav `h0`. U `LSTM` je tento stav dvouvektorový. Výstupem je `output`, což je vlastně sekvence skrytých stavů poslední vrstvy rekurentní sítě pro jednotlivé kroky v čase, a nový stav `hn` po provedení celého průchodu. Vše vystihuje následující obrázek.\n",
    "\n",
    "![](https://i.stack.imgur.com/SjnTl.png)\n",
    "\n",
    "Zdroj: https://stackoverflow.com/a/48305882/9418551\n",
    "\n",
    "V nejjednoušším případě máme pouze jednu vrstvu sítě a jeden krok. Potom `output` a `hn` jsou stejné. To znamená, že `output` **neprochází žádnou lineární vrstvou**, jak jsme si to ukazovali na přednáškách, tedy že $y=W^{hy}h$, kde $y$ značí `output`. Transformaci na skóre/pravděpodobnost jednotlivých znaků tedy musíme provést sami. Parametry $W^{hy}$ RNN v PyTorch nezahrnují.\n",
    "\n",
    "Vstupní tensory $x_i$ na obrázku očekávají PyTorch RNN ve tvaru `(seq, batch, dim)`, kde\n",
    "- `seq` ... jak jdou znaky ve \"věte\" za sebou\n",
    "- `batch` ... počet paralelně zpracovávaných sekvencí, nezávisle na sobě\n",
    "- `dim` ... příznaky na vstupu\n",
    "\n",
    "V našem případě jsou vstupy vektory (embeddingy) jednotlivých znaků. Například tedy: `(10, 3, 5)` by znamenalo:\n",
    "- 3 paralelně zpracovávané\n",
    "- 10-znakové věty,\n",
    "- kde každý znak reprezentuje 5dimenzionální vektor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do site posleme pouze jeden znak; tvar tensoru musi byt (seq, batch, dim), proto musime pouzit reshape\n",
    "e0 = e[0].reshape(1, 1, -1)\n",
    "e0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vytvoříme jednuduchou `torch.nn.RNN`, která na vstupu očekává vektor o rozměru 30 a bude mít skrytý vektor o rozměru 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(30, 8)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V PyTorch musíme řešit inicializaci i předávání skrytého stavu v jednotlivých krocích ručně. Umožňuje to tak větší flexibilitu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicializace skryteho stavu a vstupu\n",
    "# tensory opet musi byt tvaru (seq, batch, dim)\n",
    "h = torch.rand(1, 1, 8)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dopredny pruchod\n",
    "o, h = rnn(e0, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN vrátí vždy dvojici `(output, hidden)`, které vysvětluje obrázek výše. V tomto jednoduchém případě, kdy máme pouze jedinou vrstvu, se jedná o tensory se shodnými hodnotami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool(torch.all(o == h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyní už více samostatně. Zadefinujeme vlastní třídu, která bude řešit jednotlivé kroky sama ve svém dopředném průchodu. Vstupem tedy bude sekvence čísel, výstupem skóre jednotlivých kroků."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, voc_size, emb_dim, hidden_size, output_size, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        #################################################################\n",
    "        # ZDE DOPLNIT\n",
    "        \n",
    "        self.emb = ...\n",
    "        self.rnn = ...\n",
    "        self.fc = ..\n",
    "        \n",
    "        self.reset_hidden()\n",
    "        \n",
    "        #################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #################################################################\n",
    "        # ZDE DOPLNIT\n",
    "        \n",
    "        score = ...\n",
    "        \n",
    "        #################################################################\n",
    "        \n",
    "        return score\n",
    "\n",
    "    def reset_hidden(self):\n",
    "        \n",
    "        #################################################################\n",
    "        # ZDE DOPLNIT\n",
    "        # funkce resetuje hidden state na nuly\n",
    "        \n",
    "        self.hidden = ...\n",
    "        \n",
    "        #################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# ZDE DOPLNIT\n",
    "\n",
    "voc_size = ...\n",
    "emb_dim = ...\n",
    "hidden_dim = ...\n",
    "output_dim = ...\n",
    "\n",
    "#################################################################\n",
    "\n",
    "rnn = CharRNN(voc_size, emb_dim, hidden_dim, output_dim, num_layers=1)\n",
    "stats = ans.Stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vytvoříme si také funkci pro samplování z naší sítě. Funkce přijme model `rnn` a nějaký inicializační text `init_text` a vygeneruje text - vrací tedy string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(rnn, init_text='', maxlen=150, mode='multinomial', temperature=0.6):\n",
    "    \"\"\"\n",
    "    generuje text pomoci modelu `rnn`\n",
    "    \n",
    "    vstupy:\n",
    "        rnn ... rekurentni sit odvozena z `nn.Module`, ktera po zavolani vraci dvojici (vyst_skore, skryta_rep)\n",
    "        init_text ... inicializacni text, na ktery generovani textu navaze\n",
    "        maxlen ... maximalni delka generovaneho textu\n",
    "        mode ... zpusob vyberu nasledujiciho znaku, viz komentare v kodu\n",
    "        temperature ... vyhlazeni multinomialniho rozlozeni, viz komentare v kodu\n",
    "    \"\"\"\n",
    "    # vystupni text bude pole (na konci prevedeme zpet na str)\n",
    "    out_text = list(init_text)\n",
    "    \n",
    "    # pokud nezadan, inicializujeme nahodne, dle rozlozeni prvnich znaku\n",
    "    if not out_text:\n",
    "        k = np.random.choice(len(chars), p=p0)\n",
    "        out_text = [chars[k]]\n",
    "    \n",
    "    # vstup projedeme siti, abychom ziskali aktualni hidden stav\n",
    "    rnn.reset_hidden()\n",
    "    if len(out_text) > 1:\n",
    "        x = str2idt(out_text[:-1])\n",
    "        score = rnn(x)\n",
    "    \n",
    "    # pravdepodobnosti muzeme pocitat softmaxem\n",
    "    softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    while True:\n",
    "        # nasledujici znak je posledni znak prozatimniho vystupu\n",
    "        x = str2idt(out_text[-1])\n",
    "        \n",
    "        # dopredny pruchod\n",
    "        score = rnn(x)\n",
    "        \n",
    "        # pravdepodobnosti znaku\n",
    "        prob = softmax(score).flatten()\n",
    "        \n",
    "        # vyberem index `k` nasleduciho znaku\n",
    "        if mode == 'multinomial':\n",
    "            # nasledujici znak bude vybran dle ad hoc multinomialniho rozlozeni\n",
    "            # parametr `temperature` ... vyssi hodnota znamena nahodnejsi vysledky\n",
    "            # viz https://github.com/karpathy/char-rnn#sampling\n",
    "            k = torch.multinomial(score.flatten().div(temperature).exp(), 1)[0]\n",
    "        elif mode == 'argmax':\n",
    "            #################################################################\n",
    "            # ZDE DOPLNIT\n",
    "            \n",
    "            # nasledujici znak bude ten, jehoz pravdepodobnost vysla maximalni\n",
    "            k = ...\n",
    "            \n",
    "            #################################################################\n",
    "        elif mode == 'proportional':\n",
    "            #################################################################\n",
    "            # ZDE DOPLNIT\n",
    "            \n",
    "            # nasl. znak se vybere nahodne, ale s pravdepodobnosti proporcionalni k vystupu softmaxu\n",
    "            # napr. pokud znak 'x' ma dle softmaxu 84 %, bude s pravdepodobnosti 84 % vybran jako vstup do dalsi iterace\n",
    "            k = ...\n",
    "            \n",
    "            #################################################################\n",
    "        \n",
    "        #################################################################\n",
    "        # ZDE DOPLNIT\n",
    "        \n",
    "        # zastavit, pokud end-token\n",
    "        ...\n",
    "        \n",
    "        # pridat znak na vystup\n",
    "        ...\n",
    "        \n",
    "        # zastavit, pokud text je moc dlouhy\n",
    "        ...\n",
    "        \n",
    "        #################################################################\n",
    "    \n",
    "    return ''.join(out_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(sample(rnn, init_text='prezident', mode='multinomial'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trénování"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V každé iteraci pomocí funkce `char_tensor` vytvoříme trénovací data `(inpt, targ)`, což budou číselné indexy znaků tak, jak je definuje tabulka `chr2idx`. Budeme trénovat generování znaků, tzn. že požadovaným výstupem (label, target) `targ[i]` pro vstup `inpt[i]` je vždy následující znak `inpt[i+1]`. Vektor `targ` je tedy v tomto případě stejného rozměru jako `inpt`. Nezapomeňte na poslední znak, který má jako target `\\n` značící konec sekvence.\n",
    "\n",
    "Vyzkoušejte si na příkladu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = random.choice(lines)\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# ZDE DOPLNIT\n",
    "\n",
    "inpt = ...\n",
    "targ = ...\n",
    "\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('vstup:   {} ... {}'.format(idt2str(inpt[:10]), idt2str(inpt[-10:])))\n",
    "print('target:  {} ... {}'.format(idt2str(targ[:10]), idt2str(targ[-10:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# ZDE DOPLNIT\n",
    "\n",
    "optimizer = ...\n",
    "\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = ans.Stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "example = sample(rnn, mode='multinomial')\n",
    "max_per_epoch = 1000\n",
    "rnn.train()\n",
    "\n",
    "for epoch in range(1):\n",
    "    # data budou nahodne prehazena\n",
    "    train_lines = random.sample(lines, max_per_epoch)\n",
    "\n",
    "    # progressbar\n",
    "    pb = tqdm.tqdm_notebook(train_lines, desc='ep {:03d}'.format(len(stats)))\n",
    "    \n",
    "    stats.new_epoch()\n",
    "    \n",
    "    for it, line in enumerate(pb):\n",
    "        rnn.reset_hidden()\n",
    "    \n",
    "        #################################################################\n",
    "        # ZDE DOPLNIT\n",
    "        \n",
    "        ...\n",
    "        loss = ...\n",
    "        \n",
    "        #################################################################\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if it % 100 == 0:\n",
    "            rnn.eval()\n",
    "            example = sample(rnn)\n",
    "            rnn.train()\n",
    "        \n",
    "        stats.append_batch_stats('train', loss=float(loss))\n",
    "        pb.set_postfix(loss='{:.3f}'.format(stats.ravg('train', 'loss')), ex=example[:40])\n",
    "    \n",
    "# pripadne ulozit model\n",
    "# torch.save(rnn.state_dict(), f'lstm-{epoch:02d}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.plot_by_batch(block_len=10, right_metric=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.eval()\n",
    "for i in range(5):\n",
    "    print(sample(rnn, init_text='prezi', mode='multinomial'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
